{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f190a60-965a-4ddb-98d8-ad56917c937e",
   "metadata": {},
   "source": [
    "This notebook assumes that the texts of interest have already been converted to .tess files, with their metadata recorded in a .json file. Here, we will unitize, tokenize and lemmatize the texts, and create vector embeddings and draft translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b03a891d-b9cc-48a9-b021-d1c92926a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from tesserae.db import TessMongoConnection\n",
    "from tesserae.db.entities import Text\n",
    "from tesserae.utils import TessFile\n",
    "from tesserae.tokenizers import GreekTokenizer, LatinTokenizer\n",
    "from tesserae.unitizer import Unitizer\n",
    "\n",
    "connection = TessMongoConnection('127.0.0.1', 27017, None, None, db='maximus')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452cbba5-b932-475e-a350-a21b9fff1aef",
   "metadata": {},
   "source": [
    "# Import into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab15fc76-735b-4657-809a-fcef2b4c905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/maximus_texts.json', 'r') as f:\n",
    "    meta_maximus = json.load(f)\n",
    "with open('data/maximus_sources.json', 'r') as f:\n",
    "    meta_sources = json.load(f)\n",
    "\n",
    "#Texts\n",
    "texts = []\n",
    "for t in meta_maximus+meta_sources:\n",
    "    if len(connection.find('texts',title=t['title'],author=t['author'])) == 0:\n",
    "        texts.append(Text.json_decode(t))\n",
    "result = connection.insert(texts)\n",
    "if len(texts)>0:\n",
    "    print('Inserted {} texts.'.format(len(result.inserted_ids)))\n",
    "\n",
    "for text in texts:\n",
    "    tessfile = TessFile(text.path, metadata=text)\n",
    "    if text.language == 'greek':\n",
    "        tokenizer = GreekTokenizer(connection)\n",
    "    elif text.language == 'latin':\n",
    "        tokenizer = LatinTokenizer(connection)\n",
    "    else:\n",
    "        print('language not recognized')\n",
    "\n",
    "    #Features\n",
    "    tokens, tags, features = tokenizer.tokenize(tessfile.read(), text=tessfile.metadata)  \n",
    "    result = connection.insert(features)\n",
    "    result = connection.update(features)\n",
    "\n",
    "    #Units\n",
    "    unitizer = Unitizer()\n",
    "    lines, phrases = unitizer.unitize(tokens, tags, tessfile.metadata)\n",
    "    result = connection.insert(lines + phrases)    \n",
    "\n",
    "    #Tokens\n",
    "    chunk = 100000\n",
    "    for k in range((len(tokens)//chunk)+1):\n",
    "        result = connection.insert(tokens[k*chunk:(k+1)*chunk])\n",
    "    print(text.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c859f9b-5aab-450b-9f6d-f24f81747590",
   "metadata": {},
   "source": [
    "# Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b49688d-dc35-4d1d-b16f-ff31a5a8fd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tesserae.db.entities import Vector\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "with open('api_key.txt','r') as f:\n",
    "    api_key = f.read()\n",
    "\n",
    "def embed_text(text_id,unit_type='line',model=\"text-embedding-3-small\"):\n",
    "    text_df = pd.DataFrame(connection.aggregate('units',[{'$match': {'text': text_id, 'unit_type': unit_type}}, \n",
    "                                               {\"$project\": {\"_id\": 0, \"index\": 1, \"tags\": 1, \"unit_type\": 1, \"snippet\": 1}}],\n",
    "                                              encode=False))\n",
    "    lengths = text_df['snippet'].str.len()\n",
    "    text_df = text_df.loc[lengths>16]\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    chunk=1000\n",
    "    for k in range((len(text_df)//chunk)+1):\n",
    "        text_chunk = text_df.iloc[k*chunk:(k+1)*chunk]\n",
    "        embeddings = client.embeddings.create(input = list(text_chunk['snippet'].values), model=model).data\n",
    "        result = connection.insert([Vector(text=text_id,model=model,vector=embeddings[n].embedding,**text_chunk.T.drop('snippet').T.iloc[n]) for n in range(len(embeddings))])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a103435-0b82-4200-b1d5-86592b0e0274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Testament line\n",
      "Old Testament phrase\n",
      "Isagoge line\n",
      "Isagoge phrase\n",
      "Categories line\n",
      "Categories phrase\n"
     ]
    }
   ],
   "source": [
    "for text in connection.find('texts'):\n",
    "    if len(connection.find('vectors',text=text.id,unit_type='line')) == 0:\n",
    "        out1 = embed_text(text.id,unit_type='line')\n",
    "        print(text.title+' line')\n",
    "    if len(connection.find('vectors',text=text.id,unit_type='phrase')) == 0:\n",
    "        out2 = embed_text(text.id,unit_type='phrase')\n",
    "        print(text.title+' phrase')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
